{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec5b75e",
   "metadata": {},
   "source": [
    "## PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38657a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.dataset import ImageDataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((272, 272), interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.RandomResizedCrop(\n",
    "        (256, 256),\n",
    "        scale=(0.85, 1.0),\n",
    "        ratio=(0.9, 1.1),\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.08),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "root_train = os.path.join('..', 'data', 'ImageNetSubset')\n",
    "root_test = os.path.join('..', 'data', 'collected_dataset')\n",
    "\n",
    "dataset_train = ImageDataset(root=root_train, transform=train_transform, train=True)\n",
    "dataset_eval = ImageDataset(root=root_train, transform=eval_transform, train=True)\n",
    "dataset_test = ImageDataset(root=root_test, transform=eval_transform, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25f47c21e67b28c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train path: c:\\Users\\klugm\\Documents\\Studium\\BA_AI\\5. Semester\\xai-proj\\xai_proj_b_group_404\\data\\ImageNetSubset\n",
      "Exists? True\n",
      "Test path: c:\\Users\\klugm\\Documents\\Studium\\BA_AI\\5. Semester\\xai-proj\\xai_proj_b_group_404\\data\\collected_dataset\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "#check if paths are correct in different machines / operating systems\n",
    "\n",
    "print(\"Train path:\", os.path.abspath(root_train))\n",
    "print(\"Exists?\", os.path.exists(root_train))\n",
    "\n",
    "print(\"Test path:\", os.path.abspath(root_test))\n",
    "print(\"Exists?\", os.path.exists(root_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9469244",
   "metadata": {},
   "source": [
    "## PyTorch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Generator\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "\n",
    "n = len(dataset_train)\n",
    "train_size = int(0.8 * n)\n",
    "val_size = n - train_size\n",
    "\n",
    "train_idx, val_idx = random_split(\n",
    "    range(n),\n",
    "    [train_size, val_size],\n",
    "    generator=Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset_train,  train_idx.indices)   # augmented\n",
    "val_dataset   = Subset(dataset_eval, val_idx.indices)     # clean\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "testloader = DataLoader(dataset_test, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33181652",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "522c597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import SimpleCNN, LargerNet\n",
    "\n",
    "net = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0321d2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vorher per pip3 tensorflow installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c684dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b632e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/5 - train_loss: 2.1522  val_loss: 2.0421  val_acc: 0.2619 LR: 0.001000 F1_score: 0.2336 AUC_ROC: 0.7137\n",
      "Epoch 2/5 - train_loss: 1.9543  val_loss: 1.9525  val_acc: 0.3070 LR: 0.001000 F1_score: 0.2831 AUC_ROC: 0.7581\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m bs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\klugm\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\klugm\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\klugm\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn, optim \n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import time\n",
    "\n",
    "net = SimpleCNN()\n",
    "early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Device selection: MPS (Apple Silicon) > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "net.to(device)\n",
    "\n",
    "writer = SummaryWriter(log_dir=f'./runs/LargerNet/exp_{int(time.time())}')\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    net.train()\n",
    "    running_loss_sum = 0.0\n",
    "    train_samples = 0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        bs = inputs.size(0)\n",
    "        running_loss_sum += loss.item() * bs\n",
    "        train_samples += bs\n",
    "\n",
    "    # --- Validation ---\n",
    "    net.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_samples = 0\n",
    "    correct = 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    val_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            bs = inputs.size(0)\n",
    "            val_loss_sum += loss.item() * bs\n",
    "            val_samples += bs\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_predictions.append(predicted.cpu())\n",
    "            val_true_labels.append(labels.cpu())\n",
    "            val_probs.append(probs.cpu())\n",
    "\n",
    "    #Calculate Loss\n",
    "    avg_train_loss = running_loss_sum / train_samples\n",
    "    avg_val_loss = val_loss_sum / val_samples if val_samples > 0 else 0.0\n",
    "\n",
    "    #Calculate Accuracy\n",
    "    val_acc = correct / val_samples if val_samples > 0 else 0.0\n",
    "    \n",
    "    #Calculate F1 Score\n",
    "    y_true = torch.cat(val_true_labels).numpy()\n",
    "    y_pred = torch.cat(val_predictions).numpy()\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    #Calculate AUC-ROC\n",
    "    y_probs = torch.cat(val_probs).numpy()\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_probs, multi_class='ovr', average='weighted')\n",
    "    except ValueError:\n",
    "        roc_auc = float('nan')  # if not all classes are present in y_true\n",
    "\n",
    "    # --- TensorBoard Logging ---\n",
    "    writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val_epoch', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val_epoch', val_acc, epoch)\n",
    "    writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.add_scalar('F1_Score/val_epoch', f1_weighted, epoch)\n",
    "    writer.add_scalar('AUC_ROC/val_epoch', roc_auc, epoch)\n",
    " \n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch+1}/{num_epochs} '\n",
    "        f'- train_loss: {avg_train_loss:.4f} '\n",
    "        f' val_loss: {avg_val_loss:.4f} '\n",
    "        f' val_acc: {val_acc:.4f}'\n",
    "        f' LR: {optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "        f' F1_score: {f1_weighted:.4f}'\n",
    "        f' AUC_ROC: {roc_auc:.4f}'\n",
    "    )\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if early_stopper.early_stop(avg_val_loss):\n",
    "        print('Stopped\\nMin Validation Loss:', early_stopper.min_validation_loss)\n",
    "        break\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc0d5a",
   "metadata": {},
   "source": [
    "## Model Testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e69551",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_predictions = torch.Tensor()\n",
    "all_labels = torch.Tensor()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predictions = torch.cat((all_predictions, predicted.cpu()))\n",
    "        all_labels = torch.cat((all_labels, labels.cpu()))\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c04d4b",
   "metadata": {},
   "source": [
    "## Ergebnisse Speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHTUNG: Dateinamen anpassen! sonst wird altes Modell Ã¼berschrieben\n",
    "PATH = 'image_net_2511xx_2.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir='c:\\Users\\elias\\Downloads\\events.out.tfevents.1764768203.db0cf8b52daa.47.3'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
